{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220503 Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXaqHqkphbQQ",
        "outputId": "b7c6991d-4f90-4261-8a6b-cbccd6418261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ],
      "source": [
        "pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iFLRpSQBwE-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e002d415-bda5-4873-af3f-205a602d815f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vocab_size = 32000\n",
        "# vocab의 크기를 의미한다. 적을 수록 \"단어\" 단위로, 클 수록 \"음절\" 단위로 나뉘어진다.\n",
        "my_limit_alphabet = 6000\n",
        "# 모든 알파벳을 커버할 수 있도록 하여, [UNK] 빈도를 줄이기 위해 6000을 선택했다.\n",
        "my_special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "# tokenizer 에서 사용될 special_tokens이다. 필수 토큰은 위와 같다.\n",
        "user_defined_symbols = ['[BOS]','[EOS]']\n",
        "# 이제부터는 부가적인 토큰이다. 문장의 시작과 끝을 알리는 토큰을 추가했다.\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "# KoELECTRA Github를 참고하여, unused 토큰을 약 200개 추가했다. 범용성을 높일 수 있다.\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "my_special_tokens = my_special_tokens + user_defined_symbols"
      ],
      "metadata": {
        "id": "av6yNhPPtUXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\"./data/train\").glob(\"sqlCorpus.txt\")]\n",
        "# 학습에 사용될 Corpus들을 넣으면 된다. \n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=True,\n",
        "    strip_accents=True, \n",
        "    # 만약 cased model이라면 반드시 False로 해야한다, 또한 한글의 경우 cased model로 하면 글자가 자소분리된다.\n",
        "    lowercase=True,\n",
        "    # 대소문자 구분 여부를 의미한다. 한글의 경우 무의미하므로 신경쓰지 않아도 된다.\n",
        "    wordpieces_prefix=\"##\"\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files=paths,\n",
        "    limit_alphabet=my_limit_alphabet,\n",
        "    vocab_size=my_vocab_size,\n",
        "    min_frequency=5,\n",
        "    # pair가 5회이상 등장할시에만 학습\n",
        "    show_progress=True,\n",
        "    # 진행과정 출력 여부\n",
        "    special_tokens=my_special_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "bewVTb7htXJ_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"./resultVocabJson.json\".format(my_limit_alphabet, my_vocab_size),True)\n",
        "# config, model등 저장하지 않고 vocab 정보만 json 형태로 저장\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "vocab_path = \"./resultVocabJson.json\"\n",
        "# save의 결과로 추출된 파일 경로\n",
        "vocab_file = './resultVocabTxt.txt'\n",
        "# vocab.txt 형태로 저장할 경로\n",
        "f = open(vocab_file,'w',encoding='utf-8')\n",
        "with open(vocab_path) as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "    for item in json_data[\"model\"][\"vocab\"].keys():\n",
        "        f.write(item+'\\n')\n",
        "\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "OtC0or-07OhF"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}